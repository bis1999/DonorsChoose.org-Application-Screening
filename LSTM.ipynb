{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Preprocess all the Data we have in DonorsChoose <a href='https://drive.google.com/drive/folders/1MIwK7BQMev8f5CbDDVNLPaFGB32pFN60'>Dataset</a> use train.csv\n",
    "2. Combine 4 essay's into one column named - 'preprocessed_essays'. \n",
    "3. After step 2 you have to train 3 types of models as discussed below. \n",
    "4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a href='https://datascience.stackexchange.com/a/20192'>this</a> for using auc as a metric \n",
    "5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n",
    "6. You can use any one of the optimizers and choice of Learning rate and momentum, resources: <a href='http://cs231n.github.io/neural-networks-3/'>cs231n class notes</a>, <a href='https://www.youtube.com/watch?v=hd_KFJ5ktUc'>cs231n class video</a>. \n",
    "7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in .ipynb notebook and PDF. \n",
    "8. Use Categorical Cross Entropy as Loss to minimize.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-1\n",
    "\n",
    "Build and Train deep neural network as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/w395Yk9.png'>\n",
    "ref: https://i.imgur.com/w395Yk9.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n",
    "- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n",
    "- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n",
    "- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "resources = pd.read_csv(\"resources.csv\")\n",
    "quantity = resources[\"quantity\"]\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "pred_data = pd.read_csv(\"preprocessed_data.csv\")\n",
    "pred_data[\"quantity\"] = quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hasnum(string):\n",
    "    return any(i.isdigit() for i in string)\n",
    "pred_data[\"digit_present_train\"] = train[\"project_resource_summary\"].map(hasnum).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [\"teacher_number_of_previously_posted_projects\",\"quantity\",\"digit_present_train\",\"price\"]\n",
    "pred_data.to_csv(\"Final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse import hstack\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM,Bidirectional\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Reshape,Concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LeakyReLU\n",
    "import keras\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train,cv and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_data = pd.read_csv(\"Final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69918, 11)\n",
      "(21850, 11)\n",
      "(69918,)\n",
      "(21850,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y = pred_data[\"project_is_approved\"]\n",
    "X = pred_data.drop(\"project_is_approved\",axis=1)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(X_train, y_train, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train[\"essay\"].tolist())\n",
    "\n",
    "## Tokenizing the essays\n",
    "train_token = tokenizer.texts_to_sequences(X_train[\"essay\"])\n",
    "test_token = tokenizer.texts_to_sequences(X_test[\"essay\"])\n",
    "cv_token = tokenizer.texts_to_sequences(X_cv[\"essay\"])\n",
    "\n",
    "### padding the tokenize \n",
    "max_length =300\n",
    "X_train_pad = pad_sequences(train_token, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(test_token , maxlen=max_length, padding='post')\n",
    "X_cv_padd = pad_sequences(cv_token, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing for Embedding \n",
    "https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "glove_vector_saved = open(\"glove_vectors\",\"rb\") # Pepearing for embedding layers\n",
    "glove_words = pickle.load(glove_vector_saved)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 300)) \n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove_words.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Numerical and Categorical features \n",
    "https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values\n",
    "https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(train,cv,test,col):\n",
    "    vectorizer = LabelEncoderExt() \n",
    "    vectorizer.fit(train[col].values) \n",
    "\n",
    "    X_train_col = vectorizer.transform(train[col].values) \n",
    "    X_cv_col = vectorizer.transform(cv[col].values)\n",
    "    X_test_col = vectorizer.transform(test[col].values)\n",
    "    \n",
    "    return X_train_col,X_cv_col,X_test_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# School State\n",
    "X_train_state_ohe,X_cv_state_ohe,X_test_state_ohet = ohe(X_train,X_cv,X_test,\"school_state\")\n",
    "# Teacher Prefix\n",
    "X_train_pref_ohe,X_cv_pref_ohe,X_test_pref_ohe = ohe(X_train,X_cv,X_test,\"teacher_prefix\")\n",
    "# Project_grade_category\n",
    "X_train_pgc_ohe,X_cv_pgc_ohe,X_test_pgc_ohe = ohe(X_train,X_cv,X_test,\"project_grade_category\")\n",
    "\n",
    "#clean_categories\n",
    "X_train_cc_ohe,X_cv_cc_ohe,X_test_cc_ohet = ohe(X_train,X_cv,X_test,\"clean_categories\")\n",
    "\n",
    "#clean_subcategories\n",
    "X_train_cs_ohe,X_cv_cs_ohe,X_test_cs_ohe= ohe(X_train,X_cv,X_test,\"clean_subcategories\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_input_train = np.concatenate((X_train['quantity'].values.reshape(-1,1),X_train['price'].values.reshape(-1,1),X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1),X_train[\"digit_present_train\"].values.reshape(-1,1)),axis=1)\n",
    "rem_input_test = np.concatenate((X_test['quantity'].values.reshape(-1,1),X_test['price'].values.reshape(-1,1),X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1),X_test[\"digit_present_train\"].values.reshape(-1,1)), axis=1)\n",
    "rem_input_cv = np.concatenate((X_cv['quantity'].values.reshape(-1,1),X_cv['price'].values.reshape(-1,1),X_cv['teacher_number_of_previously_posted_projects'].values.reshape(-1,1),X_cv[\"digit_present_train\"].values.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler().fit(rem_input_train)\n",
    "rem_input_train_norm = ss.transform(rem_input_train)\n",
    "rem_input_cv_norm = ss.transform(rem_input_cv)\n",
    "rem_input_test_norm = ss.transform(rem_input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary_train = to_categorical(y_train)\n",
    "y_binary_cv = to_categorical(y_cv)\n",
    "y_binary_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = cat_vars =[\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"]\n",
    "\n",
    "cat_sz = {}\n",
    "cat_embed_sz = {}\n",
    "for cat in cat_vars:\n",
    "    cat_sz[cat] = X_train[cat].nunique()\n",
    "    cat_embed_sz[cat] = min(50,cat_sz[cat]//2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = [] # Storing all the input layers here , since there are multiple inputs \n",
    "concat = [] # Storing all the cancatenate layers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding + LSTM for text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(max_length,), name = \"text_input\") # eassyinputs \n",
    "ins.append(text_input) # appending to inputs \n",
    "embedding = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length,trainable=False)(text_input) # Embedding layer\n",
    "\n",
    "lstm= LSTM(128,kernel_initializer='glorot_normal',recurrent_dropout=0.5,return_sequences=True)(embedding)\n",
    "\n",
    "f1= Flatten()(lstm)\n",
    "concat.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layers for categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars =[\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"] # Categorical \n",
    "\n",
    "cat_sz = {}\n",
    "cat_embed_sz = {}\n",
    "\n",
    "for cat in cat_vars:\n",
    "    cat_sz[cat] = X_train[cat].nunique()\n",
    "    cat_embed_sz[cat] = min(50,cat_sz[cat]//2+1)\n",
    "    \n",
    "    \n",
    "for cat in cat_vars:\n",
    "    x = Input((1,), name=cat)\n",
    "    ins.append(x)\n",
    "    x = Embedding(cat_sz[cat]+1, cat_embed_sz[cat], input_length=1)(x)\n",
    "    x = Flatten(name=\"flat_\"+cat)(x)\n",
    "    concat.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical feature Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_input_layer = Input(shape=(4,),name = \"rem_input_layer\")\n",
    "ins.append(rem_input_layer)\n",
    "rem_input_dense = Dense(64, activation='relu',kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(rem_input_layer)\n",
    "concat.append(rem_input_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Concatenate()(concat)\n",
    "\n",
    "x= Dense(256,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x= Dropout(0.6)(x)\n",
    "x= Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x= Dropout(0.5)(x)\n",
    "x= Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "output=Dense(2, activation='softmax',name = \"output\")(x)\n",
    "model_l = Model(inputs=ins, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model_l.compile(optimizer=adam, loss='categorical_crossentropy',metrics=[auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "filepath=\"model1.h5\"\n",
    "checkpoint_1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "tbCallBack = TensorBoard(log_dir='./logmod1', histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "earlystop_1 = EarlyStopping(monitor = 'val_loss', \n",
    "                            mode=\"min\",\n",
    "                            min_delta = 0, \n",
    "                            patience = 2,\n",
    "                            verbose = 1,\n",
    "                            restore_best_weights = True)\n",
    "\n",
    "reduce_lr_1 = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 1, verbose = 1, min_delta = 0.0001)\n",
    "callbacks_list=[tbCallBack,earlystop_1,reduce_lr_1]\n",
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_1= model_l.fit({'text_input': X_train_pad, \n",
    "                        'school_state': X_train_state_ohe, \n",
    "                        'project_grade_category': X_train_pgc_ohe,\n",
    "                        'clean_categories': X_train_cc_ohe,\n",
    "                        'clean_subcategories':X_train_cs_ohe,\n",
    "                        'teacher_prefix':X_train_pref_ohe, \n",
    "                        'rem_input_layer':rem_input_train_norm},\n",
    "          y_binary_train,\n",
    "          epochs=30, batch_size=batch_size,verbose=1, \n",
    "                       validation_data=({'text_input':X_cv_padd, \n",
    "                                         'school_state': X_cv_state_ohe,\n",
    "                                         'project_grade_category': X_cv_pgc_ohe,\n",
    "                                         'clean_categories': X_cv_cc_ohe,\n",
    "                                         'clean_subcategories':X_cv_cs_ohe, \n",
    "                                         'teacher_prefix':X_cv_pref_ohe,\n",
    "                                         'rem_input_layer':rem_input_cv_norm},\n",
    "          y_binary_cv),callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"log_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "1. Train the TF-IDF on the Train data <br>\n",
    "2. Get the idf value for each word we have in the train data. <br>\n",
    "3. Remove the low idf value and high idf value words from our data. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very frequent words and very very rare words don't give much information. (you can plot a box plots and take only the idf scores within IQR range and corresponding words)<br>\n",
    "4. Train the LSTM after removing the Low and High idf value words. (In model-1 Train on total data but in Model-2 train on data after removing some words based on IDF values)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tf_ess = tfidf.fit_transform(X_train[\"essay\"]) # Tf-Idf\n",
    "dict_ess = dict(zip(tfidf.get_feature_names(),list(tfidf.idf_)))\n",
    "tfidf_dataframe = pd.DataFrame(list(dict_ess.items()),columns= [\"Words\",\"Values\"]) # converting tfidf into dataframe\n",
    "tfidf_dataframe = tfidf_dataframe.sort_values(by=\"Values\")             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40639</th>\n",
       "      <td>students</td>\n",
       "      <td>1.007724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28289</th>\n",
       "      <td>nannan</td>\n",
       "      <td>1.045410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37030</th>\n",
       "      <td>school</td>\n",
       "      <td>1.161311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28187</th>\n",
       "      <td>my</td>\n",
       "      <td>1.246864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24529</th>\n",
       "      <td>learning</td>\n",
       "      <td>1.363673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Words    Values\n",
       "40639  students  1.007724\n",
       "28289    nannan  1.045410\n",
       "37030    school  1.161311\n",
       "28187        my  1.246864\n",
       "24529  learning  1.363673"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Boxplot for Tfidf\")\n",
    "plt.boxplot(tfidf_dataframe[\"Values\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 percentile\n",
      "[6.1812831]\n",
      "10 percentile\n",
      "[7.45461234]\n",
      "25 percentile\n",
      "[9.44704251]\n",
      "75 percentile\n",
      "[11.46194553]\n",
      "99 percentile\n",
      "[11.46194553]\n"
     ]
    }
   ],
   "source": [
    "print(\"5 percentile\")\n",
    "print(np.percentile(tfidf_dataframe[\"Values\"],[5]))\n",
    "print(\"10 percentile\")\n",
    "print(np.percentile(tfidf_dataframe[\"Values\"],[10]))\n",
    "print(\"25 percentile\")\n",
    "print(np.percentile(tfidf_dataframe[\"Values\"],[25]))\n",
    "print(\"75 percentile\")\n",
    "print(np.percentile(tfidf_dataframe[\"Values\"],[75]))\n",
    "print(\"99 percentile\")\n",
    "print(np.percentile(tfidf_dataframe[\"Values\"],[99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting features more than 9 and less than 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_25_75= tfidf_dataframe[(tfidf_dataframe[\"Values\"] >= 9.44704251 ) & (tfidf_dataframe[\"Values\"] <= 11.46194553)]\n",
    "Words = tfidf_25_75[\"Words\"].tolist()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_words = tfidf_dataframe[\"Words\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tf = Tokenizer()\n",
    "tokenizer_tf.fit_on_texts(fil_words)\n",
    "\n",
    "## Tokenizing the essays\n",
    "train_token_tf = tokenizer.texts_to_sequences(X_train[\"essay\"])\n",
    "test_token_tf = tokenizer.texts_to_sequences(X_test[\"essay\"])\n",
    "cv_token_tf = tokenizer.texts_to_sequences(X_cv[\"essay\"])\n",
    "\n",
    "vocab_size = len(tokenizer_tf.word_index) + 1\n",
    "\n",
    "\n",
    "### padding the tokenize \n",
    "max_length =300\n",
    "X_train_pad_tf = pad_sequences(train_token_tf, maxlen=max_length, padding='post')\n",
    "X_test_pad_tf = pad_sequences(test_token_tf , maxlen=max_length, padding='post')\n",
    "X_cv_padd_tf = pad_sequences(cv_token_tf, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_tf = zeros((vocab_size, 300))\n",
    "for word, i in tokenizer_tf.word_index.items():\n",
    "    embedding_vector_tf = glove_words.get(word)\n",
    "    if embedding_vector_tf is not None:\n",
    "        embedding_matrix_tf[i] = embedding_vector_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_tf = [] # Storing all the input layers here \n",
    "concat_tf = [] # Storing all the cancatenate layers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = Input(shape=(max_length,), name = \"text_input\") # eassyinputs \n",
    "ins_tf.append(text_input) # appending to inputs \n",
    "E1 = Embedding(vocab_size, 300, weights=[embedding_matrix_tf], input_length=max_length,trainable=False)(text_input) # Embedding layer\n",
    "\n",
    "lstm= LSTM(128,kernel_initializer='glorot_normal',recurrent_dropout=0.5,return_sequences=True)(E1)\n",
    "\n",
    "f1= Flatten()(lstm)\n",
    "concat_tf.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars =[\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"clean_categories\",\"clean_subcategories\"] # Categorical \n",
    "\n",
    "cat_sz = {}\n",
    "cat_embed_sz = {}\n",
    "\n",
    "for cat in cat_vars:\n",
    "    cat_sz[cat] = X_train[cat].nunique()\n",
    "    cat_embed_sz[cat] = min(50,cat_sz[cat]//2+1)\n",
    "    \n",
    "    \n",
    "for cat in cat_vars:\n",
    "    x = Input((1,), name=cat)\n",
    "    ins_tf.append(x)\n",
    "    x = Embedding(cat_sz[cat]+1, cat_embed_sz[cat], input_length=1)(x)\n",
    "    x = Flatten(name=\"flat_\"+cat)(x)\n",
    "    concat_tf.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_input_layer = Input(shape=(4,),name = \"rem_input_layer\")\n",
    "ins_tf.append(rem_input_layer)\n",
    "rem_input_dense = Dense(64, activation='relu',kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(rem_input_layer)\n",
    "concat_tf.append(rem_input_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Concatenate()(concat_tf)\n",
    "\n",
    "x= Dense(256,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x= Dropout(0.6)(x)\n",
    "x= Dense(128,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x= Dropout(0.5)(x)\n",
    "x= Dense(64,kernel_initializer='glorot_normal',kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "output_tf=Dense(2, activation='softmax',name = \"output\")(x)\n",
    "model_2 = Model(inputs=ins_tf, outputs=output_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model_2.compile(optimizer=adam, loss='categorical_crossentropy',metrics=[auroc])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import *\n",
    "filepath=\"model1.h5\"\n",
    "checkpoint_1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, mode='max')\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "tbCallBack = TensorBoard(log_dir='./logmod1', histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "earlystop_1 = EarlyStopping(monitor = 'val_loss', \n",
    "                            mode=\"min\",\n",
    "                            min_delta = 0, \n",
    "                            patience = 2,\n",
    "                            verbose = 1,\n",
    "                            restore_best_weights = True)\n",
    "\n",
    "reduce_lr_1 = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 1, verbose = 1, min_delta = 0.0001)\n",
    "callbacks_list=[tbCallBack,earlystop_1,reduce_lr_1]\n",
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2= model_2.fit({'text_input': X_train_pad_tf, \n",
    "                        'school_state': X_train_state_ohe, \n",
    "                        'project_grade_category': X_train_pgc_ohe,\n",
    "                        'clean_categories': X_train_cc_ohe,\n",
    "                        'clean_subcategories':X_train_cs_ohe,\n",
    "                        'teacher_prefix':X_train_pref_ohe, \n",
    "                        'rem_input_layer':rem_input_train_norm},\n",
    "          y_binary_train,\n",
    "          epochs=30, batch_size=batch_size,verbose=1, \n",
    "                       validation_data=({'text_input':X_cv_padd_tf, \n",
    "                                         'school_state': X_cv_state_ohe,\n",
    "                                         'project_grade_category': X_cv_pgc_ohe,\n",
    "                                         'clean_categories': X_cv_cc_ohe,\n",
    "                                         'clean_subcategories':X_cv_cs_ohe, \n",
    "                                         'teacher_prefix':X_cv_pref_ohe,\n",
    "                                         'rem_input_layer':rem_input_cv_norm},\n",
    "          y_binary_cv),callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"log_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Here we see that after 7th epoch the model tends to overfit </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/fkQ8nGo.png'>\n",
    "ref: https://i.imgur.com/fkQ8nGo.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- __input_seq_total_text_data__: <br>\n",
    "<pre>\n",
    "    . Use text column('essay'), and use the Embedding layer to get word vectors. <br>\n",
    "    . Use given predefined glove word vectors, don't train any word vectors. <br>\n",
    "    . Use LSTM that is given above, get the LSTM output and Flatten that output. <br>\n",
    "    . You are free to preprocess the input text as you needed. <br>\n",
    "</pre>\n",
    "- __Other_than_text_data__:<br>\n",
    "<pre>\n",
    "    . Convert all your Categorical values to onehot coded and then concatenate all these onehot vectors <br>\n",
    "    . Neumerical values and use <a href='https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions'>CNN1D</a> as shown in above figure. <br>\n",
    "    . You are free to choose all CNN parameters like kernel sizes, stride.<br>\n",
    "    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countohe(col):\n",
    "    vec  = CountVectorizer()\n",
    "    X_train_ht = vec.fit_transform(X_train[col])\n",
    "    X_cv_ht = vec.fit_transform(X_cv[col])\n",
    "    X_test_ht = vec.fit_transform(X_test[col])\n",
    "    return X_train_ht,X_cv_ht,X_test_ht\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = countohe(\"teacher_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_state,X_cv_state,X_test_state = countohe(\"school_state\")\n",
    "# Teacher Prefix\n",
    "X_train_pref,X_cv_pref,X_test_pref = countohe(\"teacher_prefix\")\n",
    "# Project_grade_category\n",
    "X_train_pgc,X_cv_pgc,X_test_pg = countohe(\"project_grade_category\")\n",
    "\n",
    "#clean_categories\n",
    "X_train_cc,X_cv_cc,X_test_cc = countohe(\"clean_categories\")\n",
    "\n",
    "#clean_subcategories\n",
    "X_train_cs,X_cv_cs,X_test_cs = countohe(\"clean_subcategories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### input 2\n",
    "x_train_3 = hstack((X_train_state,X_train_pref,X_train_pgc,X_train_cc, X_train_cs,rem_input_train_norm)).todense()\n",
    "x_train_3 = np.array(x_train_3).reshape(x_train_3.shape[0],x_train_3.shape[1],1)\n",
    "\n",
    "\n",
    "\n",
    "x_test_3 = hstack((X_test_state,X_test_pref,X_test_pg,X_test_cc, X_test_cs,rem_input_test_norm)).todense()\n",
    "x_test_3 = np.array(x_test_3).reshape(x_test_3.shape[0],x_test_3.shape[1],1)\n",
    "\n",
    "x_CV_3 = hstack((X_cv_state,X_cv_pref,X_cv_pgc,X_cv_cc, X_cv_cs,rem_input_cv_norm)).todense()\n",
    "x_CV_3 = np.array(x_CV_3).reshape(x_CV_3.shape[0],x_CV_3.shape[1],1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69918, 103, 1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17480, 4)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rem_input_cv_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## essay Text\n",
    "input_a = Input(shape=(max_length,))\n",
    "E1 = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length,trainable=False)(input_a)# Embedding layer\n",
    "X1 = Dropout(0.3)(E1)\n",
    "X1 = LSTM(128,return_sequences=True)(X1)\n",
    "X1 = Flatten()(X1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Categorical and Numerical\n",
    "input_2 = Input(shape=(103,1))\n",
    "X2 = Conv1D(filters=128,kernel_size=3,strides=1)(input_2)\n",
    "X2 = Conv1D(filters=64,kernel_size=3,strides=1)(X2)\n",
    "X2 = Flatten()(X2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mergeing the inputs\n",
    "concat = Concatenate(axis=1)([X1,X2])\n",
    "X_f = Dense(128,activation='relu',kernel_regularizer=l2(0.0001))(concat)\n",
    "X_f = Dropout(0.5)(X_f)\n",
    "X_f = Dense(64,activation='relu',kernel_regularizer=l2(0.0001))(X_f)\n",
    "X_f = Dropout(0.5)(X_f)\n",
    "X_f = BatchNormalization()(X_f)\n",
    "X_f = Dense(32,activation='relu',kernel_regularizer=l2(0.0001))(X_f)\n",
    "X_f = Dropout(0.5)(X_f)\n",
    "output = Dense(2, activation = 'softmax')(X_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 =Model([input_a,input_2],output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.001)\n",
    "model_3.compile(optimizer=adam, loss='categorical_crossentropy',metrics=[auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [X_train_pad, x_train_3]\n",
    "x_test  = [X_cv_padd, x_CV_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17480, 103, 1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_CV_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src=\"log3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> So far the model does not overfit and the loss also converges </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_3.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
